<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40" />
  <meta name="author" content="Valentin De Bortoli">
  <meta name="description" content="Phd student in Applied Mathematics">

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  


  

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="vdb">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="vdb">
  

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="/publication/soul/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="vdb">
  <meta property="og:url" content="/publication/texture_soul/">
  <meta property="og:title" content="Schrodinger bridge
 | vdb">
  <meta property="og:description" content="Schrodinger bridge.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-09-23T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2017-09-23T00:00:00&#43;00:00">
  

  

  <title> Diffusion Schr&ouml;dinger Bridge with Applications to Score-Based Generative Modeling
 | vdb</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">vdb</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  


  <div class="container pub-title">
    <h1 itemprop="name">Diffusion Schr&ouml;dinger Bridge with Applications to Score-Based Generative Modeling
</h1>
    <span class="pub-authors" itemprop="author">
      
      De Bortoli, Thornton, Heng, Doucet      
      
    </span>
    <span class="pull-right">
      
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Review%20of%20wavelet-based%20unsupervised%20texture%20segmentation%2c%20advantage%20of%20adaptive%20wavelets&amp;url=%2fpublication%2fwavelet_based_unsupervised_texture_segmentation%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpublication%2fwavelet_based_unsupervised_texture_segmentation%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpublication%2fwavelet_based_unsupervised_texture_segmentation%2f&amp;title=Review%20of%20wavelet-based%20unsupervised%20texture%20segmentation%2c%20advantage%20of%20adaptive%20wavelets"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpublication%2fwavelet_based_unsupervised_texture_segmentation%2f&amp;title=Review%20of%20wavelet-based%20unsupervised%20texture%20segmentation%2c%20advantage%20of%20adaptive%20wavelets"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Review%20of%20wavelet-based%20unsupervised%20texture%20segmentation%2c%20advantage%20of%20adaptive%20wavelets&amp;body=%2fpublication%2fwavelet_based_unsupervised_texture_segmentation%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


    </span>
  </div>

  <div class="article-container">
    <a name="top"></a>
    <ul>
  <li><a href="#generative">What is score-based generative modeling?</a></li>
  <li><a href="#schrodinger">What is a  Schr&ouml;dinger Bridge?</a></li>
  <li><a href="#twodim">Two dimensional examples</a></li>
  <li><a href="#MNIST">MNIST dataset</a></li>
  <li><a href="#celeba_exploration">CelebA exploration</a></li>
  <li><a href="#dataset_interpolation">Dataset interpolation</a></li>
    </ul>

    <b>Disclaimer:</b> the paper is available
    on <a href="https://arxiv.org/abs/2106.01357">arxiv</a> and was accepted for
    a spotlight presentation at NeurIPS 2021. The code is available
    on <a href="https://github.com/JTT94/diffusion_schrodinger_bridge">Github</a>.
    
    <h3>What is score-based generative modeling?</h3>
    <a name="generative"></a>
    
    In <i>generative modeling</i> we are interested into designing algorithms
    which transform a given distribution $p_{\mathrm{prior}}$ into a given data
    distribution $p_{\mathrm{data}}$. We have access to the data distribution
    only through samples. In the last years, generative modeling has become a
    classical task in machine learning and image processing. There is a flurry
    of frameworks which aim at solving this problem such as Energy-Based models,
    Generative Adversarial Networks, normalizing flows, Variational
    Auto-encoders or diffusion score-matching techniques. The latter has shown
    great promises in outperforming GANs in term of visual
    quality, <a href=#ref>[1]</a> and can be recast as the discretization of
    some diffusion process and thus is also appealing from a mathematical point
    of view.

    <br>
    <br>
    Let us recall some of the basics concepts of score-based generative
    modeling, see
    also <a href="http://yang-song.github.io/blog/2021/score/">Yang Song
    blogpost</a> for an introduction or the original
    papers <a href="#ref">[2,3]</a>.  If we have access to a
    (Ornstein-Ulhenbeck) diffusion $$ \mathrm{d} \mathbf{X}_t = -\alpha
    \mathbf{X}_t \mathrm{d} t + \sqrt{2} \mathrm{d} \mathbf{B}_t \ , \qquad
    \mathbf{X}_0 \sim p_{\mathrm{data}} \ ,$$ with $\alpha > 0$. The process
    $(\mathbf{X}_t)_{t \in [0,T]}$ is interpreted as a <i>noising</i> process,
    perturbing the data distribution $p_{\mathrm{data}}$. We denote
    $p_{\mathrm{prior}}$ the invariant distribution of the noising process, here
    $p_{\mathrm{prior}} = \mathcal{N}(0,1/\alpha)$. If $T > 0$ is large enough
    then the distribution of $\mathbf{X}_T$ is close to $p_{\mathrm{prior}}$ due
    to the ergodicity of the Ornstein-Ulhenbeck process. Then one can
    (approximately) sample from $p_{\mathrm{data}}$ by first sampling from
    $p_{\mathrm{prior}}$ and reversing the dynamics of $(\mathbf{X}_t)_{t \in
    [0,T]}$. Surprisingly this time-reversal operation leads to another
    diffusion process with explicit drift and diffusion
    matrix <a href="#ref">[4]</a>.  $$ \mathrm{d} \mathbf{Y}_t = \{\alpha
    \mathbf{Y}_t + 2 \nabla \log p_{T-t}(\mathbf{Y}_{t}) \}\mathrm{d} t +
    \sqrt{2} \mathrm{d} \mathbf{B}_t \ , \qquad \mathbf{Y}_0 \sim
    p_{\mathrm{prior}} \ ,$$ where $p_t$ is the density of $\mathbf{X}_t$. A
    generative model can be obtained using an Euler-Maruyama discretization of
    the previous diffusion process. We end up with the following Markov chain.
    $$ Y_{k+1}^\star = Y_k^\star + \gamma_{k+1} \{\alpha Y_k^\star + 2 \nabla
    \log p_{T- t_k}(Y_k^\star)\} + \sqrt{2 \gamma_{k+1}} \ ,$$ where
    $\{\gamma_{k+1}\}_{k=0}^{N-1}$ is a sequence of stepsizes and $t_k =
    \sum_{j=0}^{k-1} \gamma_{j+1}$.  The Markov chain $\{Y_k^\star\}_{k=0}^N$
    cannot be computed in practice because we do not have access to the
    logarithmic gradient (Stein score) $\nabla \log p_{t}$.  In score-based
    generative modeling techniques this score is approximated by a neural
    network $s_{\theta^\star}$ solving the following variational problem
    $$\theta^\star = \mathrm{argmin}_\theta \mathrm{E}[\| \mathbf{Z}/\sigma_t +
    s_{\theta}(\mathbf{X}_t) \|^2] \ \, $$ where we recall that solutions of the
    Ornstein-Ulhenbeck process can be written as $\mathbf{X}_t = m_t
    \mathbf{X}_0 + \sigma_t \mathbf{Z}$, with $\mathbf{Z} \sim
    \mathcal{N}(0,1)$, $m_t = \exp[-\alpha t]$ and $\sigma_t^2 = (1 - \exp[-2
    \alpha t])/\alpha$. The variational formulation for $s_{\theta^\star}$ can
    be obtained using the following formula $$ \nabla \log p_t (x) =
    \mathrm{E}[(m_t \mathbf{X}_0 - \mathbf{X}_t)/\sigma_t^2 | \mathbf{X}_t=x] =
    -\mathrm{E}[\mathbf{Z}/\sigma_t | \mathbf{X}_t=x] $$ Finally the generative
    model is given by the following Markov chain $$ Y_{k+1} = Y_k + \gamma_{k+1}
    \{\alpha Y_k + 2 s_{\theta^\star}(T- t_k, Y_k)\} + \sqrt{2 \gamma_{k+1}} \
    .$$ These ideas are at the core of every score-based generative modeling
    method.

    <br>
    <p style="text-align:center;">
    <img src="celeba_large.gif" width="40%" /><br> Image extracted
    from <a href="http://yang-song.github.io/blog/2021/score/">Yang Song
    blog</a>. Generative model for  CelebA.
    </p>

    One of the main limitation of score-based generative modeling is that they
    require a large number of step sizes so that the initial forward dynamics is
    close to the distribution $p_{\mathrm{prior}}$ and small enough stepsizes
    so that the neural network approximation holds.

    <br>
    <br> In the next paragraph we introduce our main contribution: <b>Diffusion
    Schr&ouml;dinger Bridge</b>, a new algorithm which generalizes existing
    score-based methods allowing to significantly reduce the number of stepsizes
    needed in order to define score-based generative modeling. This contribution
    also removes the limitation for $p_{\mathrm{prior}}$ to be Gaussian and
    enables future applications in high-dimensional optimal transport.
    
    
    <h3>What is a  Schr&ouml;dinger Bridge?</h3>
    <a name="schrodinger"></a>
    <a href="#top">&#10548; Go back</a> <br>

The Schr&ouml;dinger Bridge (SB) problem is a classical problem appearing in
applied mathematics, optimal control and probability; see <a href="#ref">[5, 6,
7]</a>.  In the discrete-time setting, it takes the following (dynamic)
form. Consider as reference diffusion $(\mathbf{X}_t)_{t \in [0,T]}$ with
distribution $\mathbb{P}$ describing the process adding noise to the data.  We aim to
find $\pi^\star$ such that $\pi^\star_0 = p_{\mathrm{data}}$ and
$\pi^\star_T = p_{\mathrm{prior}}$ and minimize the Kullback-Leibler
divergence between $\pi^\star$ and $\mathbb{P}$. More precisely $$ \pi^\star =
\mathrm{argmin} \ \{\mathrm{KL}(\pi|\mathbb{P})\ , \ \pi_0 = p_{\mathrm{data}} \ , \ \pi_N =
p_{\mathrm{prior}}\}$$ In this work we introduce <b>Diffusion Schr&ouml;dinger
Bridge</b> (DSB), a new algorithm which uses score-matching
approaches <a href="#href">[2,3]</a> to approximate the <i>Iterative
Proportional Fitting</i> (IPF) algorithm, an iterative method to find the
solutions of the SB problem. DSB can be seen as a refinement of existing
score-based generative modeling methods <a href="#ref">[5, 6]</a>. In the discrete world, IPF is
also known as the <i>Sinkhorn algorithm</i>, see <a href="#href">[8]</a> for a
review.

    <img src="schrodinger_bridge.png" width="100%" />


    First, let us describe IPF. In order to find the solution of the SB problem
    IPF operates iteratively by successively solving half-bridge problems. Doing
    so, we define a sequence of distributions $(\pi^n)_{n \in \mathbb{N}}$ such
    that $$ \pi^{2n+1} = \mathrm{argmin} \{ \mathrm{KL}(\pi|\pi^{2n}) \ , \
    \pi_N = p_{\mathrm{prior}} \} \ , \\ \pi^{2n+2} = \mathrm{argmin} \{
    \mathrm{KL}(\pi|\pi^{2n+1}) \ , \ \pi_0 = p_{\mathrm{data}} \} \ , $$ with
    initial condition $\pi^0 = \mathbb{P}$ the <i>reference</i> dynamics. For large
    $n$, $\pi^n$ is close to the Schr&ouml;dinger bridge $p^\star$. Hence, a
    generative model is obtained by sampling from $\pi^{2n+1}$.


    <br>
    <br>
    We know show how this problem is related to score-based generative modeling.
    Assume that $\pi^{2n}$ is the measure associated with the diffusion $$
    \mathrm{d} \mathbf{X}_t^n = f_t^n(\mathbf{X}_t^n) \mathrm{d} t + \sqrt{2}
    \mathrm{d} \mathbf{B}_t \ , \quad \mathbf{X}_0^n \sim p_{\mathrm{data}} \ ,
    $$ then we show that $(\pi^{2n+1})^R$ (where $R$ denotes the time-reversal
    operation) is associated with the diffusion $$ \mathrm{d} \mathbf{Y}_t^n =
    b_{T-t}^n(\mathbf{Y}_t^n) \mathrm{d} t + \sqrt{2} \mathrm{d} \mathbf{B}_t \
    , \quad \mathbf{Y}_0^n \sim p_{\mathrm{prior}} $$ where $$ b_{t}^n(x) =
    -f_t^n(x) + 2 \nabla \log p_t^n(x) \ , $$ with $p_t^n$ the density of
    $\pi_t^{2n}$.  Repeating this procedure we obtain that $\pi^{2n+2}$
    is associated with the diffusion $$ \mathrm{d} \mathbf{X}_t^{n+1} =
    f_t^{n+1}(\mathbf{X}_t^{n+1}) \mathrm{d} t + \sqrt{2} \mathrm{d}
    \mathbf{B}_t \ , \quad \mathbf{X}_0^{n+1} \sim p_{\mathrm{data}} \ , $$
    where $$ f_{t}^{n+1}(x) = -b_t^n(x) + 2 \nabla \log q_t^n(x) \ , $$ with
    $q_t^n$ the density of $\pi_t^{2n+1}$.  We can then iterate this
    procedure. Of course we can not sample from these dynamics directly and we
    discretize them using Euler-Maruyama approximation. The logarithmic
    gradients are then approximated using score-matching techniques (although
    for memory reasons we do not approximate the scores but the <i>drift</i>
    functions). This discretization and variational approximation define
    our <b>Diffusion Schr&ouml;dinger Bridge</b> algorithm.

    <br>
    <br>    
    <b> Some advantages </b>
       <ul>
  <li>Contrary to existing score-based generative modeling methods which require
  the reference process to converge to $p_{\mathrm{prior}}$ the convergence of
  our algorithm is determined by the convergence of the IPF.</li>
  <li>The DSB algorithm can be used on top of existing algorithms. Hence, our
  method can be seen as a refinement of original score-based generative models
  and all techniques used to improve the quality/speed of theses methods can be
  implemented for DSB.</li>
  <li>DSB does not require $p_{\mathrm{prior}}$ to be Gaussian. In fact we only
  require having access to samples from $p_{\mathrm{prior}}$ which can be another dataset. In
  particular we are able to perform <a href="#dataset_interpolation">dataset
  interpolation</a>. This paves the way for further applications for high
  dimensional optimal transport.</li>
   </ul>

       <b> Current limitation </b>
       <ul>
	 <li>DSB does not achieve state-of-the-art generative modeling results
	 (yet) due to compute limitations as we use existing architectures in
	 order to parameterize our score approximations. These architectures are
	 deep and notably instable (for instance they require the use of
	 exponential moving average). This requires careful selection of the
	 parameters of DSB.</li>
       </ul>

       <br>
       <b>Disclaimer: </b> in this short paragraph I tried to give a
       presentation of our work from the <i>continuous-time</i> point of
       view. In the paper we follow a similar presentation but with
       a <i>discrete-time</i> point of view. The two formulations define the
       same algorithm but the discrete-time formulation avoids the need of
       reverse-time diffusions at the expense of a Gaussian approximation.

       <br><br>
In the next sections we show some of our results on <a href="#twodim">two dimensional examples</a>, 
  <a href="#MNIST">MNIST</a>, 
  <a href="#celeba_exploration">CelebA</a> and 
  <a href="#dataset_interpolation">dataset interpolation</a>.

	   

<h3>Two dimensional examples</h3>
<a name="twodim"></a>
<a href="#top">&#10548; Go back</a> <br>

For each of the row, we show the target density on the left and an animated plot
of the DSB iterations on the right. Here the prior density $p_{\mathrm{prior}}$
is given by a Gaussian density with zero mean and covariance matrix $\sigma
\mathrm{Id}$ where $\sigma$ is the variance computed from the target
dataset. We fix the number of stepsizes to $20$.

<p style="text-align:center;">
  <img src="gif/circle_init.png" width="300" />
  <img src="gif_schro/circle_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/moon_init.png" width="300" />
  <img src="gif_schro/moon_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/pinwheel_init.png" width="300" />
  <img src="gif_schro/pinwheel_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/mixture_init.png" width="300" />
  <img src="gif_schro/mixture_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/swiss_init.png" width="300" />
  <img src="gif_schro/swiss_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/scurve_init.png" width="300" />
  <img src="gif_schro/scurve_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/8gaussians_init.png" width="300" />
  <img src="gif_schro/8gaussians_0123.gif" width="300" /> 
</p>

<p style="text-align:center;">
  <img src="gif/checker_init.png" width="300" />
  <img src="gif_schro/checker_0123.gif" width="300" /> 
</p>

<h3>MNIST</h3>
<a name="MNIST"></a>
<a href="#top">&#10548; Go back</a> <br>

First we show some of samples obtained with our DSB algorithm (obtained with 30 steps in the backward dynamics).
    <p style="text-align:center;">
      <img src="original.png" width="350" />
      <img src="high_quality_mnist.png" width="350" />
      <br>
      Original dataset (left) and generated samples (right).
  </p>
Then, we present an animated plot of the DSB iterations (with 10 steps in the
backward dynamics). Note how the quality of the samples improve through the DSB
iterations.
<p style="text-align:center;">
  <img src="gif/mnist_out.gif" />
  </p>

<h3>CelebA exploration</h3>
<a name="celeba_exploration"></a>
<a href="#top">&#10548; Go back</a> <br>

In our work we apply DSB for the generation of CelebA.
<p style="text-align:center;">
  <img src="celeba.gif" width="250"/>
  </p>
Here we show some latent
space exploration. The Gaussian random variables in the generative models are
fixed and therefore the transformation is deterministic. In this animation we
follow an Ornstein-Ulhenbeck process into the latent space and observe its
transformation by this deterministic mapping in the image space.

<p style="text-align:center;">
  <img src="celeba_exploration.gif" width="400"/>
  </p>


<h3>Dataset interpolation</h3>
<a name="dataset_interpolation"></a>
<a href="#top">&#10548; Go back</a> <br>


Finally, we present some dataset interpolations experiments in two dimensions,

    <p style="text-align:center;">
      <img src="scurve_init.png" width="32%" />
      <img src="scurve2moon.gif" width="32%" />
      <img src="moon_init.png" width="32%" />
      <br>
  </p>


        <p style="text-align:center;">
      <img src="moon_init.png" width="32%" />
      <img src="moon2scurve.gif" width="32%" />
      <img src="scurve_init.png" width="32%" />
      <br>
  </p>


	    <p style="text-align:center;">
      <img src="circle_init.png" width="32%" />
      <img src="circle2scurve.gif" width="32%" />
      <img src="scurve_init.png" width="32%" />
      <br>
  </p>

	    as well as between MNIST (handwritten digits) and EMNIST (handwritten letters).

	    <p style="text-align:center;">
      <img src="out_inter.gif" width="40%" />
      <br>
  </p>
	    
	    

<h3>References</h3>
<a name="ref"></a>
<a href="#top">&#10548; Go back</a> <br> <br>

[1] Prafulla Dhariwal and Alex Nichol
<i>Diffusion models beat GAN on Image Synthesis</i>
In: arxiv preprint:2105.05233

<br><br>

[2] Yang Song and Stefano Ermon
       <i>Generative modeling by estimating gradients of the data distribution</i>
       In: Advances in Neural Information Processing Systems 2019
<br>
<br>

[3] Jonathan Ho, Ajay Jain and Pieter Abbeel
      <i>Denoising diffusion probabilistic models</i>
       In: Advances in Neural Information Processing Systems 2020

<br>
<br>

[4] Hans F&ouml;llmer
       <i>Random fields and diffusion processes</i>
       In: École d'été de Probabilités de Saint-Flour 1985-1987
<br>
<br>

[5] Christian Léonard 
       <i>A survey of the Schr&ouml;dinger problem and some of its connections with optimal transport</i>
       In: Discrete & Continuous Dynamical Systems-A 2014
<br>
<br>

[6] Yongxin Chen, Tryphon Georgiou and Michele Pavon
       <i>Optimal Transport in Systems and Control</i>
       In: Annual Review of Control, Robotics, and Autonomous Systems 2020
<br>
<br>

[7] Aapo Hyv&auml;rinen and Peter Dayan
       <i>Estimation of non-normalized statistical models by score matching</i>
       In: Journal of Machine Learning Research 2005
<br>
<br>


[8] Gabriel Peyre and Marco Cuturi <i>Computational Optimal Transport</i>
    In:  Foundations and Trends in Machine Learning 2019

          </div>
        </div>
      </div>
      <div class="col-sm-1"></div>
    </div>
    <div class="visible-xs space-below"></div>

    <div class="space-below"></div>

    <div class="article-style"></div>

    


  </div>
</div>

<div class="container">
  <nav>
  <ul class="pager">
    

    
  </ul>
</nav>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 De Bortoli &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

